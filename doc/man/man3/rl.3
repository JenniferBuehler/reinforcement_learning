.TH "rl" 3 "Wed Oct 28 2015" "LearningAlgorithms" \" -*- nroff -*-
.ad l
.nh
.SH NAME
rl \- 
.SH SYNOPSIS
.br
.PP
.SS "Classes"

.in +1c
.ti -1c
.RI "class \fBActionAlgorithm\fP"
.br
.RI "\fIAn algorithm to operate on one Action object\&. \fP"
.ti -1c
.RI "class \fBActionBase\fP"
.br
.RI "\fIBase class for all action types to be performed\&. This must include ALL possible actions in a domain within the SAME implementing subclass\&. \fP"
.ti -1c
.RI "class \fBActionGenerator\fP"
.br
.RI "\fIIterates through all possible action by generating each possible action and applying an \fBActionAlgorithm\fP to each one\&. Knowledge of underlying domain (e\&.g\&. grid world actions up,down\&.\&.\&.) is to be used to generate actions\&. \fP"
.ti -1c
.RI "class \fBActionValuePair\fP"
.br
.RI "\fIA pair of action and value, which can be used as a key\&. The key will be the Action, therefore this datatype has to support the < operator\&. The value is simply associated with the action and plays no role for the key\&. \fP"
.ti -1c
.RI "class \fBDecayLearningRate\fP"
.br
.ti -1c
.RI "class \fBDomain\fP"
.br
.RI "\fIBase class for all domains, implementing certain aspects which need domain knowledge\&. \fP"
.ti -1c
.RI "class \fBExploration\fP"
.br
.RI "\fIInterface for an exploration function, taking an utility value (e\&.g\&. q-value) and a frequency of an action tried in a state as input, and returning a utility value\&. \fP"
.ti -1c
.RI "class \fBGridDomain\fP"
.br
.RI "\fIthe \fBDomain\fP of the grid world\&. \fP"
.ti -1c
.RI "class \fBGridWorldActionGenerator\fP"
.br
.RI "\fIGenerates actions for the grid world\&. \fP"
.ti -1c
.RI "class \fBGridWorldState\fP"
.br
.RI "\fIState of the grid world\&. \fP"
.ti -1c
.RI "class \fBGridWorldStateGenerator\fP"
.br
.RI "\fIGenerates states for the grid world\&. \fP"
.ti -1c
.RI "class \fBGridWorldTransition\fP"
.br
.RI "\fISimple transition function which is known a priori for the grid world and does not have to be learned\&. \fP"
.ti -1c
.RI "class \fBLearnableTransitionMap\fP"
.br
.RI "\fIA transition map which can be learned\&. Implements \fBTransitionStlMap\fP for holding probabilities, and uses another \fBTransitionStlMap\fP (with StateActionStateValue = integer) to count the total number \fBexperienceTransition()\fP was called on a transition\&. \fP"
.ti -1c
.RI "class \fBLearningController\fP"
.br
.RI "\fIInterface to learning algorithms\&. After setting the initial state of the environment for the learner, it can be used to update the state each time step\&. Internally, each state will lead to a recommended action by the learner, which in turn can be used to transfer the system into a new state\&. This will have to be done outside this class, e\&.g\&. in a subclass of \fBDomain\fP, which in turn uses the transition function to transfer between states\&. \fP"
.ti -1c
.RI "class \fBLearningRate\fP"
.br
.ti -1c
.RI "class \fBLookupPolicy\fP"
.br
.ti -1c
.RI "class \fBMappedUtility\fP"
.br
.ti -1c
.RI "class \fBMaxUtilityActionAlgorithm\fP"
.br
.ti -1c
.RI "class \fBMoveAction\fP"
.br
.RI "\fIAction to move in the grid world\&. \fP"
.ti -1c
.RI "class \fBNoExploration\fP"
.br
.ti -1c
.RI "class \fBPolicy\fP"
.br
.ti -1c
.RI "class \fBPolicyGenerationAlgorithm\fP"
.br
.RI "\fIGenerates a policy out of a utility and transition function\&. \fP"
.ti -1c
.RI "class \fBPolicyInitialisation\fP"
.br
.ti -1c
.RI "class \fBPolicyIterationController\fP"
.br
.ti -1c
.RI "class \fBPolicyIterationUpdate\fP"
.br
.RI "\fIImplements one iteration update for policy iteration algorithm\&. method \fBpreApplication()\fP has to be called, before this update is applied to all states of the domain\&. method isChanged() can be used to see whether no changes have been made on the policy during an update\&. \fP"
.ti -1c
.RI "class \fBQLearningController\fP"
.br
.ti -1c
.RI "class \fBReward\fP"
.br
.RI "\fIImplements the reward function\&. This can be a function or a lookup in a table\&. \fP"
.ti -1c
.RI "class \fBSelectedReward\fP"
.br
.ti -1c
.RI "class \fBSimpleExploration\fP"
.br
.ti -1c
.RI "class \fBStateActionPair\fP"
.br
.RI "\fIA pair of state and action which can be used as a key for stl maps\&. Prerequisite is that both State and Action support the < operator\&. \fP"
.ti -1c
.RI "class \fBStateAlgorithm\fP"
.br
.RI "\fIAn algorithm to operate on one State object\&. \fP"
.ti -1c
.RI "class \fBStateBase\fP"
.br
.RI "\fIBase class for all states possible in a domain\&. This must include ALL possible states in a domain within the SAME implementing subclass\&. \fP"
.ti -1c
.RI "class \fBStateGenerator\fP"
.br
.RI "\fIIterates through all possible states by generating each possible state (or a random set of states) and applying a \fBStateAlgorithm\fP to each state\&. Knowledge of underlying domain (e\&.g\&. grid world) is to be used to generate states\&. \fP"
.ti -1c
.RI "class \fBTransition\fP"
.br
.ti -1c
.RI "class \fBTransitionStlMap\fP"
.br
.ti -1c
.RI "class \fBUtility\fP"
.br
.RI "\fIImplements the utility function which expresses the utility of a state\&. This can be a function or a lookup in a table\&. \fP"
.ti -1c
.RI "class \fBValueIterationController\fP"
.br
.ti -1c
.RI "class \fBValueIterationUpdate\fP"
.br
.in -1c
.SS "Functions"

.in +1c
.ti -1c
.RI "template<class State , class Action > std::shared_ptr< \fBPolicy\fP< State, Action > > \fBpolicyIteration\fP (std::shared_ptr< \fBUtility\fP< State > > u, std::shared_ptr< \fBPolicy\fP< State, Action > > p, std::shared_ptr< const \fBReward\fP< State > > r, std::shared_ptr< const \fBTransition\fP< State, Action > > t, std::shared_ptr< const \fBStateGenerator\fP< State > > sg, std::shared_ptr< const \fBActionGenerator\fP< Action > > ag, float discount, unsigned int modPolicyIter=5)"
.br
.ti -1c
.RI "template<class State , class Action > std::shared_ptr< \fBUtility\fP< State > > \fBvalueIteration\fP (std::shared_ptr< \fBUtility\fP< State > > u, const std::shared_ptr< const \fBReward\fP< State > > r, const std::shared_ptr< const \fBTransition\fP< State, Action > > t, const std::shared_ptr< const \fBActionGenerator\fP< Action > > ag, const std::shared_ptr< const \fBStateGenerator\fP< State > > sg, float discount, float maxErr)"
.br
.in -1c
.SH "Function Documentation"
.PP 
.SS "template<class State , class Action > std::shared_ptr<\fBPolicy\fP<State,Action> > rl::policyIteration (std::shared_ptr< \fBUtility\fP< State > > u, std::shared_ptr< \fBPolicy\fP< State, Action > > p, std::shared_ptr< const \fBReward\fP< State > > r, std::shared_ptr< const \fBTransition\fP< State, Action > > t, std::shared_ptr< const \fBStateGenerator\fP< State > > sg, std::shared_ptr< const \fBActionGenerator\fP< Action > > ag, float discount, unsigned int modPolicyIter = \fC5\fP)"
For description of all parameters except sg see \fBPolicyIterationUpdate\fP constructor parameter description\&. 
.PP
\fBAuthor:\fP
.RS 4
Jennifer Buehler 
.RE
.PP
\fBDate:\fP
.RS 4
May 2011
.RE
.PP
\fBParameters:\fP
.RS 4
\fIu\fP correctly initialised (but still empty/unlearned) model of utility\&. This object will be changed in the course of applying this \fBStateAlgorithm\fP\&. 
.br
\fIp\fP correctly initialised (but still empty/unlearned) policy\&. This object will be changed in the course of applying this \fBStateAlgorithm\fP\&. 
.br
\fIr\fP reward function to use 
.br
\fIt\fP correctly initialised transition model\&. 
.br
\fIag\fP action generator (generates all possible actions) for domaon\&. 
.br
\fIsg\fP state generator to use 
.br
\fIdiscount\fP this is used for the policy evaluation 
.br
\fImodPolicyIter\fP for policy evaluation (modified policy iteration)\&. Indicates how many value iteration steps are performed per iteration of the policy iteration algorithm to update the utility\&. 
.RE
.PP

.SS "template<class State , class Action > std::shared_ptr<\fBUtility\fP<State> > rl::valueIteration (std::shared_ptr< \fBUtility\fP< State > > u, const std::shared_ptr< const \fBReward\fP< State > > r, const std::shared_ptr< const \fBTransition\fP< State, Action > > t, const std::shared_ptr< const \fBActionGenerator\fP< Action > > ag, const std::shared_ptr< const \fBStateGenerator\fP< State > > sg, float discount, float maxErr)"
This function applies the value iteration algorithm, given a utility function, a reward function, a transition function and implementations of \fBActionGenerator\fP and \fBStateGenerator\fP\&. For description of all parameters except sg and maxErr see \fBValueIterationUpdate\fP constructor parameter\&. 
.PP
\fBParameters:\fP
.RS 4
\fIu\fP correctly initialised (but still empty/unlearned) model of utility\&. 
.br
\fIr\fP correctly initialised model of reward\&. 
.br
\fIt\fP correctly initialised transition model\&. 
.br
\fIag\fP action generator (generates all possible actions) for domain\&. If this object is NULL, the policy is fixed, and parameter policy MUST be non-NULL! 
.br
\fIsg\fP state generator to use\&. 
.br
\fIdiscount\fP discount factor 
.br
\fImaxErr\fP maximum error allowed in the utility of any state (determines termination criterion)\&. 
.RE
.PP
\fBAuthor:\fP
.RS 4
Jennifer Buehler 
.RE
.PP
\fBDate:\fP
.RS 4
May 2011 
.RE
.PP

.SH "Author"
.PP 
Generated automatically by Doxygen for LearningAlgorithms from the source code\&.
