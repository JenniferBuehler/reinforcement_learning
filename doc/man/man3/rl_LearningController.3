.TH "rl::LearningController< Domain, StateUtilityDatatype >" 3 "Wed Oct 28 2015" "LearningAlgorithms" \" -*- nroff -*-
.ad l
.nh
.SH NAME
rl::LearningController< Domain, StateUtilityDatatype > \- Interface to learning algorithms\&. After setting the initial state of the environment for the learner, it can be used to update the state each time step\&. Internally, each state will lead to a recommended action by the learner, which in turn can be used to transfer the system into a new state\&. This will have to be done outside this class, e\&.g\&. in a subclass of \fBDomain\fP, which in turn uses the transition function to transfer between states\&.  

.SH SYNOPSIS
.br
.PP
.PP
\fC#include <Controller\&.h>\fP
.SS "Public Types"

.in +1c
.ti -1c
.RI "typedef \fBDomain\fP \fBDomainT\fP"
.br
.ti -1c
.RI "typedef \fBDomainT::DomainPtrT\fP \fBDomainPtrT\fP"
.br
.ti -1c
.RI "typedef \fBDomainT::DomainConstPtrT\fP \fBDomainConstPtrT\fP"
.br
.ti -1c
.RI "typedef \fBDomainT::StateT\fP \fBStateT\fP"
.br
.ti -1c
.RI "typedef \fBDomainT::ActionT\fP \fBActionT\fP"
.br
.ti -1c
.RI "typedef \fBPolicy\fP< \fBStateT\fP, \fBActionT\fP > \fBPolicyT\fP"
.br
.ti -1c
.RI "typedef StateUtilityDatatype \fBStateUtilityT\fP"
.br
.ti -1c
.RI "typedef \fBUtility\fP< \fBStateT\fP, \fBStateUtilityT\fP > \fBUtilityT\fP"
.br
.ti -1c
.RI "typedef \fBLearningController\fP< \fBDomainT\fP, \fBStateUtilityT\fP > \fBLearningControllerT\fP"
.br
.ti -1c
.RI "typedef std::shared_ptr< \fBLearningControllerT\fP > \fBLearningControllerPtrT\fP"
.br
.ti -1c
.RI "typedef \fBPolicyT::PolicyPtrT\fP \fBPolicyPtrT\fP"
.br
.ti -1c
.RI "typedef \fBPolicyT::PolicyConstPtrT\fP \fBPolicyConstPtrT\fP"
.br
.ti -1c
.RI "typedef \fBUtilityT::UtilityPtrT\fP \fBUtilityPtrT\fP"
.br
.ti -1c
.RI "typedef \fBUtilityT::UtilityConstPtrT\fP \fBUtilityConstPtrT\fP"
.br
.in -1c
.SS "Public Member Functions"

.in +1c
.ti -1c
.RI "\fBLearningController\fP (\fBDomainConstPtrT\fP _domain, bool _train=true)"
.br
.ti -1c
.RI "\fBLearningController\fP (const \fBLearningController\fP &o)"
.br
.ti -1c
.RI "virtual \fB~LearningController\fP ()"
.br
.ti -1c
.RI "virtual \fBActionT\fP \fBgetBestLearnedAction\fP (const \fBStateT\fP &currentState) const "
.br
.ti -1c
.RI "\fBActionT\fP \fBupdateAndGetAction\fP (const \fBStateT\fP &currState)"
.br
.ti -1c
.RI "bool \fBinitialize\fP (const \fBStateT\fP &startState)"
.br
.ti -1c
.RI "virtual void \fBresetStartState\fP (const \fBStateT\fP &startState)=0"
.br
.ti -1c
.RI "virtual bool \fBisOnlineLearner\fP ()=0"
.br
.ti -1c
.RI "virtual \fBPolicyConstPtrT\fP \fBgetPolicy\fP () const =0"
.br
.ti -1c
.RI "virtual \fBUtilityConstPtrT\fP \fBgetUtility\fP () const =0"
.br
.ti -1c
.RI "virtual short \fBfinishedLearning\fP () const =0"
.br
.ti -1c
.RI "virtual void \fBprintValues\fP (std::ostream &o) const =0"
.br
.ti -1c
.RI "virtual void \fBprintStats\fP (std::ostream &o) const "
.br
.ti -1c
.RI "void \fBsetTraining\fP (bool on)"
.br
.in -1c
.SS "Protected Member Functions"

.in +1c
.ti -1c
.RI "virtual \fBActionT\fP \fBgetBestAction\fP (const \fBStateT\fP &currentState) const =0"
.br
.ti -1c
.RI "virtual bool \fBlearnOnline\fP (const \fBStateT\fP &currState)"
.br
.ti -1c
.RI "virtual bool \fBlearnOffline\fP (const \fBStateT\fP &currState)"
.br
.ti -1c
.RI "virtual bool \fBinitializeImpl\fP (const \fBStateT\fP &startState)=0"
.br
.ti -1c
.RI "\fBLearningController\fP ()"
.br
.in -1c
.SS "Protected Attributes"

.in +1c
.ti -1c
.RI "\fBDomainConstPtrT\fP \fBdomain\fP"
.br
.ti -1c
.RI "bool \fBtrain\fP"
.br
.in -1c
.SH "Detailed Description"
.PP 

.SS "template<class Domain, typename StateUtilityDatatype = float>class rl::LearningController< Domain, StateUtilityDatatype >"
Interface to learning algorithms\&. After setting the initial state of the environment for the learner, it can be used to update the state each time step\&. Internally, each state will lead to a recommended action by the learner, which in turn can be used to transfer the system into a new state\&. This will have to be done outside this class, e\&.g\&. in a subclass of \fBDomain\fP, which in turn uses the transition function to transfer between states\&. 

Basically, there is two general differences, depending on the implementation of the subclass:
.PP
.IP "1." 4
ONLINE LEARNING Each time the controller updated (by method \fBupdateAndGetAction()\fP) a learning step is performed\&. Based on the information known until this time step, (inferred from the updated learning parameters in the learning step) the best action is chosen to be performed (this will mostly not be the actual best action in the beginning of execution)\&. An example is Q-learning, which will update the q-table each step based on an internally used reward function\&.
.IP "2." 4
OFFLINE LEARNING At initialisation time (call of method \fBinitialize()\fP), the learning is performed based on the information available (i\&.e\&. internally used model of environment, reward function, utility funcitons, which are all objects which have to be returned by \fBDomain\fP implementation)\&. Then, each time \fBupdateAndGetAction()\fP is called, the best action is chosen based on the pre-learned policy (or utility table)\&. Examples are value and policy iteration, which will learn the utility function or policy given a transition table and a reward function\&.
.PP
.PP
These two cases can be handeled generically for a simulation of the environment by using an algorithm similar as described in the following simulation code\&.
.PP
Action a; State s=initial_state; controller\&.initialize(); while (simulator_running){ a=controller\&.getAction(s); s=\&.\&.\&. transfer in new state by executing action a \&.\&. do stuff with s, e\&.g\&. set visualisation parameters \&.\&.\&. }
.PP
The method \fBisOnlineLearner()\fP can be used to determine whether this algorithm does some learning\&. This can be important for determining whether to do a quick simulation without visualisation, or a to run a learned policy\&. The method
.PP
\fBAuthor:\fP
.RS 4
Jennifer Buehler 
.RE
.PP
\fBDate:\fP
.RS 4
May 2011
.RE
.PP
\fBParameters:\fP
.RS 4
\fI\fBDomain\fP\fP the domain class (the directly used class, NOT the base class!) 
.br
\fIStateUtilityDatatype\fP the data type used to express the utility of a state\&. Default: float 
.RE
.PP

.SH "Member Typedef Documentation"
.PP 
.SS "template<class Domain, typename StateUtilityDatatype = float> typedef \fBDomainT::ActionT\fP \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::\fBActionT\fP"

.SS "template<class Domain, typename StateUtilityDatatype = float> typedef \fBDomainT::DomainConstPtrT\fP \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::\fBDomainConstPtrT\fP"

.SS "template<class Domain, typename StateUtilityDatatype = float> typedef \fBDomainT::DomainPtrT\fP \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::\fBDomainPtrT\fP"

.SS "template<class Domain, typename StateUtilityDatatype = float> typedef \fBDomain\fP \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::\fBDomainT\fP"

.SS "template<class Domain, typename StateUtilityDatatype = float> typedef std::shared_ptr<\fBLearningControllerT\fP> \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::\fBLearningControllerPtrT\fP"

.SS "template<class Domain, typename StateUtilityDatatype = float> typedef \fBLearningController\fP<\fBDomainT\fP,\fBStateUtilityT\fP> \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::\fBLearningControllerT\fP"

.SS "template<class Domain, typename StateUtilityDatatype = float> typedef \fBPolicyT::PolicyConstPtrT\fP \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::\fBPolicyConstPtrT\fP"

.SS "template<class Domain, typename StateUtilityDatatype = float> typedef \fBPolicyT::PolicyPtrT\fP \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::\fBPolicyPtrT\fP"

.SS "template<class Domain, typename StateUtilityDatatype = float> typedef \fBPolicy\fP<\fBStateT\fP,\fBActionT\fP> \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::\fBPolicyT\fP"

.SS "template<class Domain, typename StateUtilityDatatype = float> typedef \fBDomainT::StateT\fP \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::\fBStateT\fP"

.SS "template<class Domain, typename StateUtilityDatatype = float> typedef StateUtilityDatatype \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::\fBStateUtilityT\fP"

.SS "template<class Domain, typename StateUtilityDatatype = float> typedef \fBUtilityT::UtilityConstPtrT\fP \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::\fBUtilityConstPtrT\fP"

.SS "template<class Domain, typename StateUtilityDatatype = float> typedef \fBUtilityT::UtilityPtrT\fP \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::\fBUtilityPtrT\fP"

.SS "template<class Domain, typename StateUtilityDatatype = float> typedef \fBUtility\fP<\fBStateT\fP,\fBStateUtilityT\fP> \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::\fBUtilityT\fP"

.SH "Constructor & Destructor Documentation"
.PP 
.SS "template<class Domain, typename StateUtilityDatatype = float> \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::\fBLearningController\fP (\fBDomainConstPtrT\fP _domain, bool _train = \fCtrue\fP)\fC [inline]\fP, \fC [explicit]\fP"

.PP
\fBParameters:\fP
.RS 4
\fI_domain\fP the domain to be learned 
.br
\fI_train\fP initial value for training (set \fBsetTraining()\fP) 
.RE
.PP

.SS "template<class Domain, typename StateUtilityDatatype = float> \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::\fBLearningController\fP (const \fBLearningController\fP< \fBDomain\fP, StateUtilityDatatype > & o)\fC [inline]\fP"

.SS "template<class Domain, typename StateUtilityDatatype = float> virtual \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::~\fBLearningController\fP ()\fC [inline]\fP, \fC [virtual]\fP"

.SS "template<class Domain, typename StateUtilityDatatype = float> \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::\fBLearningController\fP ()\fC [inline]\fP, \fC [protected]\fP"

.SH "Member Function Documentation"
.PP 
.SS "template<class Domain, typename StateUtilityDatatype = float> virtual short \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::finishedLearning () const\fC [pure virtual]\fP"

.PP
\fBReturn values:\fP
.RS 4
\fI-2\fP the learning can never be finished because the system was not initialized 
.br
\fI-1\fP the learning process has not converged yet 
.br
\fI0\fP it is not known whether the learning has converged yet\&. This can only be determined by checking back with the domain and evaluating there whether the learning has finished (for example, in Q-Learning)\&. This function will therefore always return 0 for this Controller implementation\&. 
.br
\fI1\fP the learning has converged\&. 
.RE
.PP

.PP
Implemented in \fBrl::ValueIterationController< Domain >\fP, \fBrl::PolicyIterationController< Domain >\fP, and \fBrl::QLearningController< Domain, UtilityType >\fP\&.
.SS "template<class Domain, typename StateUtilityDatatype = float> virtual \fBActionT\fP \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::getBestAction (const \fBStateT\fP & currentState) const\fC [protected]\fP, \fC [pure virtual]\fP"
returns the best action to perform given the current state, at the current stage of the learning process\&. For online learners, this will be the action currently recommended\&. For offline learners, this will be the optimal action from the policy learned at initialisation (\fBlearnOffline()\fP)\&. 
.PP
Implemented in \fBrl::QLearningController< Domain, UtilityType >\fP, \fBrl::ValueIterationController< Domain >\fP, and \fBrl::PolicyIterationController< Domain >\fP\&.
.SS "template<class Domain, typename StateUtilityDatatype = float> virtual \fBActionT\fP \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::getBestLearnedAction (const \fBStateT\fP & currentState) const\fC [inline]\fP, \fC [virtual]\fP"
returns the best action using the policy learned so far\&. For offline learnes, this will be the value from the policy learned at inisialisation state\&. For online learners, this will be the best action depending on the current learning stage\&. IMPORTANT: Online learners may have to implement this function, as by default, it will return \fBgetBestAction()\fP! 
.PP
Reimplemented in \fBrl::QLearningController< Domain, UtilityType >\fP\&.
.SS "template<class Domain, typename StateUtilityDatatype = float> virtual \fBPolicyConstPtrT\fP \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::getPolicy () const\fC [pure virtual]\fP"
Return the learned policy 
.PP
Implemented in \fBrl::ValueIterationController< Domain >\fP, \fBrl::PolicyIterationController< Domain >\fP, and \fBrl::QLearningController< Domain, UtilityType >\fP\&.
.SS "template<class Domain, typename StateUtilityDatatype = float> virtual \fBUtilityConstPtrT\fP \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::getUtility () const\fC [pure virtual]\fP"
Return the learned utility function 
.PP
Implemented in \fBrl::ValueIterationController< Domain >\fP, \fBrl::PolicyIterationController< Domain >\fP, and \fBrl::QLearningController< Domain, UtilityType >\fP\&.
.SS "template<class Domain, typename StateUtilityDatatype = float> bool \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::initialize (const \fBStateT\fP & startState)\fC [inline]\fP"

.SS "template<class Domain, typename StateUtilityDatatype = float> virtual bool \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::initializeImpl (const \fBStateT\fP & startState)\fC [protected]\fP, \fC [pure virtual]\fP"

.PP
Implemented in \fBrl::QLearningController< Domain, UtilityType >\fP, \fBrl::ValueIterationController< Domain >\fP, and \fBrl::PolicyIterationController< Domain >\fP\&.
.SS "template<class Domain, typename StateUtilityDatatype = float> virtual bool \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::isOnlineLearner ()\fC [pure virtual]\fP"
If this method returns true, we will only have to call \fBinitialize()\fP in order to learn the utility function\&. After it has been initialised, the function transferState() can be used to transfer the state of the domain and thus use the learned policy to move around in the world\&. 
.PP
Implemented in \fBrl::ValueIterationController< Domain >\fP, \fBrl::PolicyIterationController< Domain >\fP, and \fBrl::QLearningController< Domain, UtilityType >\fP\&.
.SS "template<class Domain, typename StateUtilityDatatype = float> virtual bool \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::learnOffline (const \fBStateT\fP & currState)\fC [inline]\fP, \fC [protected]\fP, \fC [virtual]\fP"
If the implementing algorithm is an online method, this function should return true and do nothing\&. 
.PP
Reimplemented in \fBrl::ValueIterationController< Domain >\fP, and \fBrl::PolicyIterationController< Domain >\fP\&.
.SS "template<class Domain, typename StateUtilityDatatype = float> virtual bool \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::learnOnline (const \fBStateT\fP & currState)\fC [inline]\fP, \fC [protected]\fP, \fC [virtual]\fP"
updates the online learning based on the current state\&. After performing such an update, the best action recommended at the current stage of learning has to be returned with method \fBgetBestAction()\fP\&. If the learner is an offline learner, this method only returns true\&. 
.PP
Reimplemented in \fBrl::QLearningController< Domain, UtilityType >\fP\&.
.SS "template<class Domain, typename StateUtilityDatatype = float> virtual void \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::printStats (std::ostream & o) const\fC [inline]\fP, \fC [virtual]\fP"
Prints some statistics, as learning progress or sizes of tables 
.PP
Reimplemented in \fBrl::QLearningController< Domain, UtilityType >\fP\&.
.SS "template<class Domain, typename StateUtilityDatatype = float> virtual void \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::printValues (std::ostream & o) const\fC [pure virtual]\fP"
This will print the relevant values for the learning algorithm, e\&.g\&. the learned utility, policy, or q-table\&. This will vary between implementations\&. 
.PP
Implemented in \fBrl::ValueIterationController< Domain >\fP, \fBrl::PolicyIterationController< Domain >\fP, and \fBrl::QLearningController< Domain, UtilityType >\fP\&.
.SS "template<class Domain, typename StateUtilityDatatype = float> virtual void \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::resetStartState (const \fBStateT\fP & startState)\fC [pure virtual]\fP"
This method can be used to set the start state to follow with the optimal policy\&. This means that no connection with the previous state passed to updateAndGetBestAction() is assumed any more, which may be important for some online learning algorithms\&. (therefore, this method is pure virtual, to make sure subclasses consider the case that the state may be reset)\&. 
.PP
Implemented in \fBrl::ValueIterationController< Domain >\fP, \fBrl::PolicyIterationController< Domain >\fP, and \fBrl::QLearningController< Domain, UtilityType >\fP\&.
.SS "template<class Domain, typename StateUtilityDatatype = float> void \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::setTraining (bool on)\fC [inline]\fP"
Set training on or off\&. If set to off (parameter false), the learning stops and the so far explored policy will be used to determine action returned by \fBupdateAndGetAction()\fP\&. 
.SS "template<class Domain, typename StateUtilityDatatype = float> \fBActionT\fP \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::updateAndGetAction (const \fBStateT\fP & currState)\fC [inline]\fP"

.SH "Member Data Documentation"
.PP 
.SS "template<class Domain, typename StateUtilityDatatype = float> \fBDomainConstPtrT\fP \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::domain\fC [protected]\fP"

.SS "template<class Domain, typename StateUtilityDatatype = float> bool \fBrl::LearningController\fP< \fBDomain\fP, StateUtilityDatatype >::train\fC [protected]\fP"


.SH "Author"
.PP 
Generated automatically by Doxygen for LearningAlgorithms from the source code\&.
