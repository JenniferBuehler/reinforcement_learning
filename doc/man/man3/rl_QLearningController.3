.TH "rl::QLearningController< Domain, UtilityType >" 3 "Wed Oct 28 2015" "LearningAlgorithms" \" -*- nroff -*-
.ad l
.nh
.SH NAME
rl::QLearningController< Domain, UtilityType > \- 
.SH SYNOPSIS
.br
.PP
.PP
\fC#include <QLearning\&.h>\fP
.PP
Inherits \fBrl::LearningController< Domain, UtilityType >\fP\&.
.SS "Classes"

.in +1c
.ti -1c
.RI "class \fBMaxExpectedUtility\fP"
.br
.ti -1c
.RI "class \fBMaxQValue\fP"
.br
.in -1c
.SS "Public Types"

.in +1c
.ti -1c
.RI "typedef \fBDomain\fP \fBDomainT\fP"
.br
.ti -1c
.RI "typedef \fBDomainT::StateT\fP \fBStateT\fP"
.br
.ti -1c
.RI "typedef \fBDomainT::ActionT\fP \fBActionT\fP"
.br
.ti -1c
.RI "typedef \fBDomainT::RewardValueTypeT\fP \fBRewardValueTypeT\fP"
.br
.ti -1c
.RI "typedef \fBDomainT::DomainConstPtrT\fP \fBDomainConstPtrT\fP"
.br
.ti -1c
.RI "typedef UtilityType \fBUtilityDataTypeT\fP"
.br
.ti -1c
.RI "typedef \fBLearningController\fP< \fBDomainT\fP, \fBUtilityDataTypeT\fP > \fBLearningControllerT\fP"
.br
.ti -1c
.RI "typedef \fBQLearningController\fP< \fBDomainT\fP, \fBUtilityDataTypeT\fP > \fBQLearningControllerT\fP"
.br
.ti -1c
.RI "typedef \fBActionGenerator\fP< \fBActionT\fP > \fBActionGeneratorT\fP"
.br
.ti -1c
.RI "typedef unsigned int \fBFreqCntT\fP"
.br
.ti -1c
.RI "typedef \fBExploration\fP< \fBUtilityDataTypeT\fP, \fBFreqCntT\fP > \fBExplorationT\fP"
.br
.ti -1c
.RI "typedef \fBPolicy\fP< \fBStateT\fP, \fBActionT\fP > \fBPolicyT\fP"
.br
.ti -1c
.RI "typedef \fBUtility\fP< \fBStateT\fP, \fBUtilityDataTypeT\fP > \fBUtilityT\fP"
.br
.ti -1c
.RI "typedef \fBLearnableTransitionMap\fP< \fBStateT\fP, \fBActionT\fP > \fBLearnableTransitionMapT\fP"
.br
.ti -1c
.RI "typedef \fBActionGeneratorT::ActionGeneratorPtrT\fP \fBActionGeneratorPtrT\fP"
.br
.ti -1c
.RI "typedef \fBActionGeneratorT::ActionGeneratorConstPtrT\fP \fBActionGeneratorConstPtrT\fP"
.br
.ti -1c
.RI "typedef \fBPolicyT::PolicyConstPtrT\fP \fBPolicyConstPtrT\fP"
.br
.ti -1c
.RI "typedef \fBPolicyT::PolicyPtrT\fP \fBPolicyPtrT\fP"
.br
.ti -1c
.RI "typedef \fBUtilityT::UtilityConstPtrT\fP \fBUtilityConstPtrT\fP"
.br
.ti -1c
.RI "typedef \fBExplorationT::ExplorationConstPtrT\fP \fBExplorationConstPtrT\fP"
.br
.ti -1c
.RI "typedef \fBLearningRate::LearningRatePtrT\fP \fBLearningRatePtrT\fP"
.br
.ti -1c
.RI "typedef std::shared_ptr< \fBStateT\fP > \fBStatePtrT\fP"
.br
.ti -1c
.RI "typedef std::shared_ptr< const \fBStateT\fP > \fBStateConstPtrT\fP"
.br
.in -1c
.SS "Public Member Functions"

.in +1c
.ti -1c
.RI "\fBQLearningController\fP (\fBDomainConstPtrT\fP _domain, const \fBLearningRatePtrT\fP &_learnRate, const float _discount, const \fBUtilityDataTypeT\fP &_defaultQ, \fBExplorationConstPtrT\fP _exploration, float _epsilonGreedy, bool _train=true)"
.br
.ti -1c
.RI "virtual bool \fBisOnlineLearner\fP ()"
.br
.ti -1c
.RI "virtual \fBActionT\fP \fBgetBestLearnedAction\fP (const \fBStateT\fP &currentState) const "
.br
.ti -1c
.RI "virtual \fBPolicyConstPtrT\fP \fBgetPolicy\fP () const "
.br
.ti -1c
.RI "virtual \fBUtilityConstPtrT\fP \fBgetUtility\fP () const "
.br
.ti -1c
.RI "virtual void \fBresetStartState\fP (const \fBStateT\fP &startState)"
.br
.ti -1c
.RI "virtual short \fBfinishedLearning\fP () const "
.br
.ti -1c
.RI "virtual void \fBprintValues\fP (std::ostream &o) const "
.br
.ti -1c
.RI "\fBPolicyPtrT\fP \fBgetLearnedPolicy\fP () const "
.br
.ti -1c
.RI "virtual void \fBprintStats\fP (std::ostream &o) const "
.br
.in -1c
.SS "Protected Types"

.in +1c
.ti -1c
.RI "typedef \fBActionValuePair\fP< \fBActionT\fP, \fBUtilityDataTypeT\fP > \fBActionValuePairT\fP"
.br
.ti -1c
.RI "typedef std::set< \fBActionValuePairT\fP > \fBActionValueSetT\fP"
.br
.ti -1c
.RI "typedef \fBLookupPolicy\fP< \fBStateT\fP, \fBActionT\fP > \fBLookupPolicyT\fP"
.br
.ti -1c
.RI "typedef std::map< \fBStateT\fP, \fBActionValueSetT\fP > \fBQMap\fP"
.br
.ti -1c
.RI "typedef QMap::iterator \fBQMap_iterator\fP"
.br
.ti -1c
.RI "typedef QMap::const_iterator \fBQMap_const_iterator\fP"
.br
.in -1c
.SS "Protected Member Functions"

.in +1c
.ti -1c
.RI "virtual bool \fBlearnOnline\fP (const \fBStateT\fP &currentState)"
.br
.ti -1c
.RI "virtual \fBActionT\fP \fBgetBestAction\fP (const \fBStateT\fP &currentState) const "
.br
.ti -1c
.RI "virtual bool \fBinitializeImpl\fP (const \fBStateT\fP &startState)"
.br
.ti -1c
.RI "\fBActionT\fP \fBupdate\fP (const \fBStateT\fP &s, const \fBRewardValueTypeT\fP &reward)"
.br
.ti -1c
.RI "void \fBupdateFreqAndQTable\fP (const \fBStateT\fP &s, const \fBRewardValueTypeT\fP &reward)"
.br
.ti -1c
.RI "\fBActionValuePairT\fP \fBgetMaxQValue\fP (const \fBActionValueSetT\fP &avSet) const "
.br
.ti -1c
.RI "\fBQMap_const_iterator\fP \fBgetQEntry\fP (const \fBStateT\fP &s) const "
.br
.ti -1c
.RI "bool \fBgetQValue\fP (const \fBStateT\fP &s, const \fBActionT\fP &a, \fBUtilityDataTypeT\fP &actionUtility) const "
.br
.ti -1c
.RI "bool \fBgetQValue\fP (\fBQMap_const_iterator\fP qit, const \fBActionT\fP &a, \fBUtilityDataTypeT\fP &actionUtility) const "
.br
.ti -1c
.RI "\fBFreqCntT\fP \fBgetFrequency\fP (const \fBStateT\fP &s, const \fBActionT\fP &a) const "
.br
.ti -1c
.RI "void \fBprintQValues\fP (std::ostream &o) const "
.br
.ti -1c
.RI "float \fBgetAvgChange\fP () const "
.br
.ti -1c
.RI "void \fBupdateAverage\fP (\fBUtilityDataTypeT\fP diff)"
.br
.in -1c
.SS "Additional Inherited Members"
.SH "Detailed Description"
.PP 

.SS "template<class Domain, typename UtilityType = float>class rl::QLearningController< Domain, UtilityType >"
Implementation of a \fBLearningController\fP for the q learning algorithm\&.
.PP
State and Action template parameters have to be usable as a key in a map (i\&.e\&. support the < operator)\&. They should both also implement the = operator and copy constructor\&.
.PP
\fBAuthor:\fP
.RS 4
Jennifer Buehler 
.RE
.PP
\fBDate:\fP
.RS 4
May 2011 
.RE
.PP
\fBParameters:\fP
.RS 4
\fI\fBDomain\fP\fP must be the class type of the domain used (NOT the base domain class!) 
.br
\fIUtilityType\fP utility value to use for the q-table entries 
.RE
.PP

.SH "Member Typedef Documentation"
.PP 
.SS "template<class Domain , typename UtilityType  = float> typedef \fBActionGeneratorT::ActionGeneratorConstPtrT\fP \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::\fBActionGeneratorConstPtrT\fP"

.SS "template<class Domain , typename UtilityType  = float> typedef \fBActionGeneratorT::ActionGeneratorPtrT\fP \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::\fBActionGeneratorPtrT\fP"

.SS "template<class Domain , typename UtilityType  = float> typedef \fBActionGenerator\fP<\fBActionT\fP> \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::\fBActionGeneratorT\fP"

.SS "template<class Domain , typename UtilityType  = float> typedef \fBDomainT::ActionT\fP \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::\fBActionT\fP"

.SS "template<class Domain , typename UtilityType  = float> typedef \fBActionValuePair\fP<\fBActionT\fP,\fBUtilityDataTypeT\fP> \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::\fBActionValuePairT\fP\fC [protected]\fP"

.SS "template<class Domain , typename UtilityType  = float> typedef std::set<\fBActionValuePairT\fP> \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::\fBActionValueSetT\fP\fC [protected]\fP"

.SS "template<class Domain , typename UtilityType  = float> typedef \fBDomainT::DomainConstPtrT\fP \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::\fBDomainConstPtrT\fP"

.SS "template<class Domain , typename UtilityType  = float> typedef \fBDomain\fP \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::\fBDomainT\fP"

.SS "template<class Domain , typename UtilityType  = float> typedef \fBExplorationT::ExplorationConstPtrT\fP \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::\fBExplorationConstPtrT\fP"

.SS "template<class Domain , typename UtilityType  = float> typedef \fBExploration\fP<\fBUtilityDataTypeT\fP,\fBFreqCntT\fP> \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::\fBExplorationT\fP"

.SS "template<class Domain , typename UtilityType  = float> typedef unsigned int \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::\fBFreqCntT\fP"

.SS "template<class Domain , typename UtilityType  = float> typedef \fBLearnableTransitionMap\fP<\fBStateT\fP,\fBActionT\fP> \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::\fBLearnableTransitionMapT\fP"

.SS "template<class Domain , typename UtilityType  = float> typedef \fBLearningController\fP<\fBDomainT\fP,\fBUtilityDataTypeT\fP> \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::\fBLearningControllerT\fP"

.SS "template<class Domain , typename UtilityType  = float> typedef \fBLearningRate::LearningRatePtrT\fP \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::\fBLearningRatePtrT\fP"

.SS "template<class Domain , typename UtilityType  = float> typedef \fBLookupPolicy\fP<\fBStateT\fP, \fBActionT\fP> \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::\fBLookupPolicyT\fP\fC [protected]\fP"

.SS "template<class Domain , typename UtilityType  = float> typedef \fBPolicyT::PolicyConstPtrT\fP \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::\fBPolicyConstPtrT\fP"

.SS "template<class Domain , typename UtilityType  = float> typedef \fBPolicyT::PolicyPtrT\fP \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::\fBPolicyPtrT\fP"

.SS "template<class Domain , typename UtilityType  = float> typedef \fBPolicy\fP<\fBStateT\fP, \fBActionT\fP> \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::\fBPolicyT\fP"

.SS "template<class Domain , typename UtilityType  = float> typedef \fBQLearningController\fP<\fBDomainT\fP,\fBUtilityDataTypeT\fP> \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::\fBQLearningControllerT\fP"

.SS "template<class Domain , typename UtilityType  = float> typedef std::map<\fBStateT\fP,\fBActionValueSetT\fP> \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::\fBQMap\fP\fC [protected]\fP"

.SS "template<class Domain , typename UtilityType  = float> typedef QMap::const_iterator \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::\fBQMap_const_iterator\fP\fC [protected]\fP"

.SS "template<class Domain , typename UtilityType  = float> typedef QMap::iterator \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::\fBQMap_iterator\fP\fC [protected]\fP"

.SS "template<class Domain , typename UtilityType  = float> typedef \fBDomainT::RewardValueTypeT\fP \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::\fBRewardValueTypeT\fP"

.SS "template<class Domain , typename UtilityType  = float> typedef std::shared_ptr<const \fBStateT\fP> \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::\fBStateConstPtrT\fP"

.SS "template<class Domain , typename UtilityType  = float> typedef std::shared_ptr<\fBStateT\fP> \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::\fBStatePtrT\fP"

.SS "template<class Domain , typename UtilityType  = float> typedef \fBDomainT::StateT\fP \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::\fBStateT\fP"

.SS "template<class Domain , typename UtilityType  = float> typedef \fBUtilityT::UtilityConstPtrT\fP \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::\fBUtilityConstPtrT\fP"

.SS "template<class Domain , typename UtilityType  = float> typedef UtilityType \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::\fBUtilityDataTypeT\fP"

.SS "template<class Domain , typename UtilityType  = float> typedef \fBUtility\fP<\fBStateT\fP,\fBUtilityDataTypeT\fP> \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::\fBUtilityT\fP"

.SH "Constructor & Destructor Documentation"
.PP 
.SS "template<class Domain , typename UtilityType  = float> \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::\fBQLearningController\fP (\fBDomainConstPtrT\fP _domain, const \fBLearningRatePtrT\fP & _learnRate, const float _discount, const \fBUtilityDataTypeT\fP & _defaultQ, \fBExplorationConstPtrT\fP _exploration, float _epsilonGreedy, bool _train = \fCtrue\fP)\fC [inline]\fP"

.PP
\fBParameters:\fP
.RS 4
\fI_defaultQ\fP default q value for state-action pairs which haven't been encountered so far 
.br
\fI_exploration\fP exploration function to be used 
.br
\fI_domain\fP the domain to be used 
.br
\fI_learnRate\fP the learn rate to use 
.br
\fI_discount\fP discount factor 
.br
\fI_train\fP initial value for training (set \fBsetTraining()\fP) 
.br
\fI_epsilonGreedy\fP value 0\&.\&.1 indicating a probability that not best, but a random action is chosen\&. This adds to the exploration function\&. It is NOT implemented as \fBExploration\fP interface because it does not matter how often an action has been tried before\&. ALWAYS a not optimal action will be chosen, i\&.e\&. there will constantly be exploration\&. 
.RE
.PP

.SH "Member Function Documentation"
.PP 
.SS "template<class Domain , typename UtilityType  = float> virtual short \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::finishedLearning () const\fC [inline]\fP, \fC [virtual]\fP"

.PP
\fBReturn values:\fP
.RS 4
\fI-2\fP the learning can never be finished because the system was not initialized 
.br
\fI-1\fP the learning process has not converged yet 
.br
\fI0\fP it is not known whether the learning has converged yet\&. This can only be determined by checking back with the domain and evaluating there whether the learning has finished (for example, in Q-Learning)\&. This function will therefore always return 0 for this Controller implementation\&. 
.br
\fI1\fP the learning has converged\&. 
.RE
.PP

.PP
Implements \fBrl::LearningController< Domain, UtilityType >\fP\&.
.SS "template<class Domain , typename UtilityType  = float> float \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::getAvgChange () const\fC [inline]\fP, \fC [protected]\fP"

.SS "template<class Domain , typename UtilityType  = float> virtual \fBActionT\fP \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::getBestAction (const \fBStateT\fP & currentState) const\fC [inline]\fP, \fC [protected]\fP, \fC [virtual]\fP"
returns the best action to perform given the current state, at the current stage of the learning process\&. For online learners, this will be the action currently recommended\&. For offline learners, this will be the optimal action from the policy learned at initialisation (\fBlearnOffline()\fP)\&. 
.PP
Implements \fBrl::LearningController< Domain, UtilityType >\fP\&.
.SS "template<class Domain , typename UtilityType  = float> virtual \fBActionT\fP \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::getBestLearnedAction (const \fBStateT\fP & currentState) const\fC [inline]\fP, \fC [virtual]\fP"
returns the best action using the policy learned so far\&. For offline learnes, this will be the value from the policy learned at inisialisation state\&. For online learners, this will be the best action depending on the current learning stage\&. IMPORTANT: Online learners may have to implement this function, as by default, it will return \fBgetBestAction()\fP! 
.PP
Reimplemented from \fBrl::LearningController< Domain, UtilityType >\fP\&.
.SS "template<class Domain , typename UtilityType  = float> \fBFreqCntT\fP \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::getFrequency (const \fBStateT\fP & s, const \fBActionT\fP & a) const\fC [inline]\fP, \fC [protected]\fP"
Helper function to retrieve the frequency for a state-action pair\&. 
.SS "template<class Domain , typename UtilityType  = float> \fBPolicyPtrT\fP \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::getLearnedPolicy () const\fC [inline]\fP"
Returns the learned policy from applying the q-learning algorithm 
.SS "template<class Domain , typename UtilityType  = float> \fBActionValuePairT\fP \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::getMaxQValue (const \fBActionValueSetT\fP & avSet) const\fC [inline]\fP, \fC [protected]\fP"
From a set of actions with q-values associated, pick the one action which has the maximum q-value\&. 
.SS "template<class Domain , typename UtilityType  = float> virtual \fBPolicyConstPtrT\fP \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::getPolicy () const\fC [inline]\fP, \fC [virtual]\fP"
Return the learned policy 
.PP
Implements \fBrl::LearningController< Domain, UtilityType >\fP\&.
.SS "template<class Domain , typename UtilityType  = float> \fBQMap_const_iterator\fP \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::getQEntry (const \fBStateT\fP & s) const\fC [inline]\fP, \fC [protected]\fP"
Helper function, returns iterator to q-entry for the state 
.SS "template<class Domain , typename UtilityType  = float> bool \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::getQValue (const \fBStateT\fP & s, const \fBActionT\fP & a, \fBUtilityDataTypeT\fP & actionUtility) const\fC [inline]\fP, \fC [protected]\fP"
Helper function to retrieve the Q-Value for a state-action pair\&. Returns false if no such pair is in the Q-table yet, and actionUtility remains unchanged\&. If a Q-valueu exists, the function returns true and actionUtility will contain the assigned value\&. 
.SS "template<class Domain , typename UtilityType  = float> bool \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::getQValue (\fBQMap_const_iterator\fP qit, const \fBActionT\fP & a, \fBUtilityDataTypeT\fP & actionUtility) const\fC [inline]\fP, \fC [protected]\fP"
Helper function to retrieve the Q-Value for a state-action pair, given that the parameter qit points to the entry for a state\&. All actions previously tried from this state will have to be contained in this entry\&. Returns false if no such pair is in the Q-table yet, and actionUtility remains unchanged\&. If a Q-valueu exists, the function returns true and actionUtility will contain the assigned value\&. 
.SS "template<class Domain , typename UtilityType  = float> virtual \fBUtilityConstPtrT\fP \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::getUtility () const\fC [inline]\fP, \fC [virtual]\fP"
Return the learned utility function 
.PP
Implements \fBrl::LearningController< Domain, UtilityType >\fP\&.
.SS "template<class Domain , typename UtilityType  = float> virtual bool \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::initializeImpl (const \fBStateT\fP & startState)\fC [inline]\fP, \fC [protected]\fP, \fC [virtual]\fP"

.PP
Implements \fBrl::LearningController< Domain, UtilityType >\fP\&.
.SS "template<class Domain , typename UtilityType  = float> virtual bool \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::isOnlineLearner ()\fC [inline]\fP, \fC [virtual]\fP"
If this method returns true, we will only have to call \fBinitialize()\fP in order to learn the utility function\&. After it has been initialised, the function transferState() can be used to transfer the state of the domain and thus use the learned policy to move around in the world\&. 
.PP
Implements \fBrl::LearningController< Domain, UtilityType >\fP\&.
.SS "template<class Domain , typename UtilityType  = float> virtual bool \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::learnOnline (const \fBStateT\fP & currState)\fC [inline]\fP, \fC [protected]\fP, \fC [virtual]\fP"
updates the online learning based on the current state\&. After performing such an update, the best action recommended at the current stage of learning has to be returned with method \fBgetBestAction()\fP\&. If the learner is an offline learner, this method only returns true\&. 
.PP
Reimplemented from \fBrl::LearningController< Domain, UtilityType >\fP\&.
.SS "template<class Domain , typename UtilityType  = float> void \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::printQValues (std::ostream & o) const\fC [inline]\fP, \fC [protected]\fP"

.SS "template<class Domain , typename UtilityType  = float> virtual void \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::printStats (std::ostream & o) const\fC [inline]\fP, \fC [virtual]\fP"
Prints some statistics, as learning progress or sizes of tables 
.PP
Reimplemented from \fBrl::LearningController< Domain, UtilityType >\fP\&.
.SS "template<class Domain , typename UtilityType  = float> virtual void \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::printValues (std::ostream & o) const\fC [inline]\fP, \fC [virtual]\fP"
This will print the relevant values for the learning algorithm, e\&.g\&. the learned utility, policy, or q-table\&. This will vary between implementations\&. 
.PP
Implements \fBrl::LearningController< Domain, UtilityType >\fP\&.
.SS "template<class Domain , typename UtilityType  = float> virtual void \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::resetStartState (const \fBStateT\fP & startState)\fC [inline]\fP, \fC [virtual]\fP"
This method can be used to set the start state to follow with the optimal policy\&. This means that no connection with the previous state passed to updateAndGetBestAction() is assumed any more, which may be important for some online learning algorithms\&. (therefore, this method is pure virtual, to make sure subclasses consider the case that the state may be reset)\&. 
.PP
Implements \fBrl::LearningController< Domain, UtilityType >\fP\&.
.SS "template<class Domain , typename UtilityType  = float> \fBActionT\fP \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::update (const \fBStateT\fP & s, const \fBRewardValueTypeT\fP & reward)\fC [inline]\fP, \fC [protected]\fP"

.SS "template<class Domain , typename UtilityType  = float> void \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::updateAverage (\fBUtilityDataTypeT\fP diff)\fC [inline]\fP, \fC [protected]\fP"

.SS "template<class Domain , typename UtilityType  = float> void \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::updateFreqAndQTable (const \fBStateT\fP & s, const \fBRewardValueTypeT\fP & reward)\fC [inline]\fP, \fC [protected]\fP"
helper function to update the frequency table and the q-values table for the current state s\&. 

.SH "Author"
.PP 
Generated automatically by Doxygen for LearningAlgorithms from the source code\&.
