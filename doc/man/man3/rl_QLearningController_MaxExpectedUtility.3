.TH "rl::QLearningController< Domain, UtilityType >::MaxExpectedUtility" 3 "Wed Oct 28 2015" "LearningAlgorithms" \" -*- nroff -*-
.ad l
.nh
.SH NAME
rl::QLearningController< Domain, UtilityType >::MaxExpectedUtility \- 
.SH SYNOPSIS
.br
.PP
.PP
\fC#include <QLearning\&.h>\fP
.PP
Inherits \fBrl::ActionAlgorithm< ActionT >\fP\&.
.SS "Public Member Functions"

.in +1c
.ti -1c
.RI "\fBMaxExpectedUtility\fP (const \fBQLearningControllerT\fP &_qlearn, const \fBStateT\fP &_s)"
.br
.ti -1c
.RI "virtual \fB~MaxExpectedUtility\fP ()"
.br
.ti -1c
.RI "virtual bool \fBapply\fP (const \fBActionT\fP &a)"
.br
.ti -1c
.RI "bool \fBhasResult\fP ()"
.br
.ti -1c
.RI "\fBActionValuePairT\fP \fBgetBestAction\fP ()"
.br
.in -1c
.SS "Additional Inherited Members"
.SH "Detailed Description"
.PP 

.SS "template<class Domain, typename UtilityType = float>class rl::QLearningController< Domain, UtilityType >::MaxExpectedUtility"
Finds the action with the maximum expected utility, that is, finds argmax_over_a{ expl(Q[s,a],freq[s,a]) } where expl is the exploration function, and freq is the frequency of action a tried from state s\&.
.PP
Only X percent of the time, this maximum action is NOT found, but instead a random action is returned (where X = QLearningController::epsilonGreedy), thereby adding an epsilon-greedy strategy\&.
.PP
IMPORTANT: This class should only be used internally, while the qlearning object passed in constructor is valid and unchanged 
.SH "Constructor & Destructor Documentation"
.PP 
.SS "template<class Domain , typename UtilityType  = float> \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::MaxExpectedUtility::MaxExpectedUtility (const \fBQLearningControllerT\fP & _qlearn, const \fBStateT\fP & _s)\fC [inline]\fP, \fC [explicit]\fP"

.PP
\fBParameters:\fP
.RS 4
\fI_qlearn\fP the \fBQLearningController\fP object\&. Reference is kept internally! 
.br
\fI_s\fP the state from which the optimal action will be chosen 
.RE
.PP

.SS "template<class Domain , typename UtilityType  = float> virtual \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::MaxExpectedUtility::~MaxExpectedUtility ()\fC [inline]\fP, \fC [virtual]\fP"

.SH "Member Function Documentation"
.PP 
.SS "template<class Domain , typename UtilityType  = float> virtual bool \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::MaxExpectedUtility::apply (const \fBActionT\fP & a)\fC [inline]\fP, \fC [virtual]\fP"

.PP
Implements \fBrl::ActionAlgorithm< ActionT >\fP\&.
.SS "template<class Domain , typename UtilityType  = float> \fBActionValuePairT\fP \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::MaxExpectedUtility::getBestAction ()\fC [inline]\fP"
Returns the action found with the best utility, given the set exploration function\&. In addition to the exploration funciton, a random action is chosen X percent of the time instead of the best action found, witch probability QLearningController::epsilonGreedy (epsilon-greedy exploration)
.PP
See also \fBhasResult()\fP 
.SS "template<class Domain , typename UtilityType  = float> bool \fBrl::QLearningController\fP< \fBDomain\fP, UtilityType >::MaxExpectedUtility::hasResult ()\fC [inline]\fP"


.SH "Author"
.PP 
Generated automatically by Doxygen for LearningAlgorithms from the source code\&.
