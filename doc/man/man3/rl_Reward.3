.TH "rl::Reward< State, Value >" 3 "Wed Oct 28 2015" "LearningAlgorithms" \" -*- nroff -*-
.ad l
.nh
.SH NAME
rl::Reward< State, Value > \- Implements the reward function\&. This can be a function or a lookup in a table\&.  

.SH SYNOPSIS
.br
.PP
.PP
\fC#include <Reward\&.h>\fP
.PP
Inherited by \fBrl::SelectedReward< State, Value >\fP\&.
.SS "Public Types"

.in +1c
.ti -1c
.RI "typedef Value \fBValueT\fP"
.br
.ti -1c
.RI "typedef State \fBStateT\fP"
.br
.ti -1c
.RI "typedef \fBReward\fP< \fBStateT\fP, \fBValueT\fP > \fBRewardT\fP"
.br
.ti -1c
.RI "typedef std::shared_ptr< \fBRewardT\fP > \fBRewardPtrT\fP"
.br
.ti -1c
.RI "typedef std::shared_ptr< const \fBRewardT\fP > \fBRewardConstPtrT\fP"
.br
.in -1c
.SS "Public Member Functions"

.in +1c
.ti -1c
.RI "\fBReward\fP ()"
.br
.ti -1c
.RI "\fBReward\fP (const \fBReward\fP &o)"
.br
.ti -1c
.RI "virtual \fB~Reward\fP ()"
.br
.ti -1c
.RI "virtual \fBValueT\fP \fBgetReward\fP (const State &s) const =0"
.br
.ti -1c
.RI "virtual \fBValueT\fP \fBgetOptimisticReward\fP () const =0"
.br
.in -1c
.SH "Detailed Description"
.PP 

.SS "template<class State, typename Value = float>class rl::Reward< State, Value >"
Implements the reward function\&. This can be a function or a lookup in a table\&. 


.PP
\fBParameters:\fP
.RS 4
\fIState\fP template class for State description\&. Prerequisite: Must support < operator, an be uniquely identifiable, i\&.e\&. suitable to use in a std::map as key\&. Even if the \fBReward\fP is being calculated as a function by the underlying implementation, it is to be made sure that a map-lookup can be used as well for each state\&.
.br
\fIValue\fP the value type to be used as reward 
.RE
.PP
\fBAuthor:\fP
.RS 4
Jennifer Buehler 
.RE
.PP
\fBDate:\fP
.RS 4
May 2011 
.RE
.PP

.SH "Member Typedef Documentation"
.PP 
.SS "template<class State , typename Value  = float> typedef std::shared_ptr<const \fBRewardT\fP> \fBrl::Reward\fP< State, Value >::\fBRewardConstPtrT\fP"

.SS "template<class State , typename Value  = float> typedef std::shared_ptr<\fBRewardT\fP> \fBrl::Reward\fP< State, Value >::\fBRewardPtrT\fP"

.SS "template<class State , typename Value  = float> typedef \fBReward\fP<\fBStateT\fP,\fBValueT\fP> \fBrl::Reward\fP< State, Value >::\fBRewardT\fP"

.SS "template<class State , typename Value  = float> typedef State \fBrl::Reward\fP< State, Value >::\fBStateT\fP"

.SS "template<class State , typename Value  = float> typedef Value \fBrl::Reward\fP< State, Value >::\fBValueT\fP"

.SH "Constructor & Destructor Documentation"
.PP 
.SS "template<class State , typename Value  = float> \fBrl::Reward\fP< State, Value >::\fBReward\fP ()\fC [inline]\fP"

.SS "template<class State , typename Value  = float> \fBrl::Reward\fP< State, Value >::\fBReward\fP (const \fBReward\fP< State, Value > & o)\fC [inline]\fP"

.SS "template<class State , typename Value  = float> virtual \fBrl::Reward\fP< State, Value >::~\fBReward\fP ()\fC [inline]\fP, \fC [virtual]\fP"

.SH "Member Function Documentation"
.PP 
.SS "template<class State , typename Value  = float> virtual \fBValueT\fP \fBrl::Reward\fP< State, Value >::getOptimisticReward () const\fC [pure virtual]\fP"
gets an optimistic estimate of the reward, usually this would be the maximum reward possible in any state\&. 
.PP
Implemented in \fBrl::SelectedReward< State, Value >\fP\&.
.SS "template<class State , typename Value  = float> virtual \fBValueT\fP \fBrl::Reward\fP< State, Value >::getReward (const State & s) const\fC [pure virtual]\fP"
Gets the reward given a state 
.PP
Implemented in \fBrl::SelectedReward< State, Value >\fP\&.

.SH "Author"
.PP 
Generated automatically by Doxygen for LearningAlgorithms from the source code\&.
