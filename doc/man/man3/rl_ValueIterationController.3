.TH "rl::ValueIterationController< Domain >" 3 "Wed Oct 28 2015" "LearningAlgorithms" \" -*- nroff -*-
.ad l
.nh
.SH NAME
rl::ValueIterationController< Domain > \- 
.SH SYNOPSIS
.br
.PP
.PP
\fC#include <ValueIteration\&.h>\fP
.PP
Inherits \fBrl::LearningController< Domain, float >\fP\&.
.SS "Public Types"

.in +1c
.ti -1c
.RI "typedef \fBDomain\fP \fBDomainT\fP"
.br
.ti -1c
.RI "typedef \fBDomainT::DomainPtrT\fP \fBDomainPtrT\fP"
.br
.ti -1c
.RI "typedef \fBDomainT::DomainConstPtrT\fP \fBDomainConstPtrT\fP"
.br
.ti -1c
.RI "typedef float \fBUtilityDataTypeT\fP"
.br
.ti -1c
.RI "typedef \fBDomainT::StateT\fP \fBStateT\fP"
.br
.ti -1c
.RI "typedef \fBDomainT::ActionT\fP \fBActionT\fP"
.br
.ti -1c
.RI "typedef \fBLearningController\fP< \fBDomainT\fP, \fBUtilityDataTypeT\fP > \fBLearningControllerT\fP"
.br
.ti -1c
.RI "typedef \fBLearningControllerT::UtilityT\fP \fBUtilityT\fP"
.br
.ti -1c
.RI "typedef \fBLearningControllerT::PolicyT\fP \fBPolicyT\fP"
.br
.ti -1c
.RI "typedef \fBStateGenerator\fP< \fBStateT\fP > \fBStateGeneratorT\fP"
.br
.ti -1c
.RI "typedef \fBActionGenerator\fP< \fBActionT\fP > \fBActionGeneratorT\fP"
.br
.ti -1c
.RI "typedef \fBSelectedReward\fP< \fBStateT\fP > \fBSelectedRewardT\fP"
.br
.ti -1c
.RI "typedef \fBTransition\fP< \fBStateT\fP, \fBActionT\fP > \fBTransitionT\fP"
.br
.ti -1c
.RI "typedef \fBUtilityT::UtilityPtrT\fP \fBUtilityPtrT\fP"
.br
.ti -1c
.RI "typedef \fBUtilityT::UtilityConstPtrT\fP \fBUtilityConstPtrT\fP"
.br
.ti -1c
.RI "typedef \fBTransitionT::TransitionConstPtrT\fP \fBTransitionConstPtrT\fP"
.br
.ti -1c
.RI "typedef \fBActionGeneratorT::ActionGeneratorConstPtrT\fP \fBActionGeneratorConstPtrT\fP"
.br
.ti -1c
.RI "typedef \fBPolicyT::PolicyPtrT\fP \fBPolicyPtrT\fP"
.br
.ti -1c
.RI "typedef \fBPolicyT::PolicyConstPtrT\fP \fBPolicyConstPtrT\fP"
.br
.ti -1c
.RI "typedef \fBStateGeneratorT::StateGeneratorConstPtrT\fP \fBStateGeneratorConstPtrT\fP"
.br
.ti -1c
.RI "typedef \fBMaxUtilityActionAlgorithm\fP< \fBStateT\fP, \fBActionT\fP > \fBMaxUtilityActionAlgorithmT\fP"
.br
.ti -1c
.RI "typedef class \fBPolicyGenerationAlgorithm\fP< \fBStateT\fP, \fBActionT\fP, \fBUtilityDataTypeT\fP > \fBPolicyGenerationAlgorithmT\fP"
.br
.ti -1c
.RI "typedef PolicyGenerationAlgorithmT::PolicyGenerationAlgorithmPtrT \fBPolicyGenerationAlgorithmPtrT\fP"
.br
.in -1c
.SS "Public Member Functions"

.in +1c
.ti -1c
.RI "\fBValueIterationController\fP (\fBDomainConstPtrT\fP _domain, float _defaultUtility, float _discount, float _maxErr, bool _train=true)"
.br
.ti -1c
.RI "virtual \fB~ValueIterationController\fP ()"
.br
.ti -1c
.RI "virtual bool \fBisOnlineLearner\fP ()"
.br
.ti -1c
.RI "virtual \fBPolicyConstPtrT\fP \fBgetPolicy\fP () const "
.br
.ti -1c
.RI "virtual \fBUtilityConstPtrT\fP \fBgetUtility\fP () const "
.br
.ti -1c
.RI "virtual void \fBresetStartState\fP (const \fBStateT\fP &startState)"
.br
.ti -1c
.RI "virtual short \fBfinishedLearning\fP () const "
.br
.ti -1c
.RI "virtual void \fBprintValues\fP (std::ostream &o) const "
.br
.in -1c
.SS "Protected Types"

.in +1c
.ti -1c
.RI "typedef \fBMappedUtility\fP< \fBStateT\fP > \fBMappedUtilityT\fP"
.br
.in -1c
.SS "Protected Member Functions"

.in +1c
.ti -1c
.RI "virtual bool \fBlearnOffline\fP (const \fBStateT\fP &currState)"
.br
.ti -1c
.RI "virtual \fBActionT\fP \fBgetBestAction\fP (const \fBStateT\fP &currentState) const "
.br
.ti -1c
.RI "virtual bool \fBinitializeImpl\fP (const \fBStateT\fP &startState)"
.br
.ti -1c
.RI "\fBValueIterationController\fP ()"
.br
.in -1c
.SS "Additional Inherited Members"
.SH "Detailed Description"
.PP 

.SS "template<class Domain>class rl::ValueIterationController< Domain >"
\fBLearningController\fP implementation for the value iteration algorithm\&. 
.PP
\fBAuthor:\fP
.RS 4
Jennifer Buehler 
.RE
.PP
\fBDate:\fP
.RS 4
May 2011 
.RE
.PP

.SH "Member Typedef Documentation"
.PP 
.SS "template<class Domain > typedef \fBActionGeneratorT::ActionGeneratorConstPtrT\fP \fBrl::ValueIterationController\fP< \fBDomain\fP >::\fBActionGeneratorConstPtrT\fP"

.SS "template<class Domain > typedef \fBActionGenerator\fP<\fBActionT\fP> \fBrl::ValueIterationController\fP< \fBDomain\fP >::\fBActionGeneratorT\fP"

.SS "template<class Domain > typedef \fBDomainT::ActionT\fP \fBrl::ValueIterationController\fP< \fBDomain\fP >::\fBActionT\fP"

.SS "template<class Domain > typedef \fBDomainT::DomainConstPtrT\fP \fBrl::ValueIterationController\fP< \fBDomain\fP >::\fBDomainConstPtrT\fP"

.SS "template<class Domain > typedef \fBDomainT::DomainPtrT\fP \fBrl::ValueIterationController\fP< \fBDomain\fP >::\fBDomainPtrT\fP"

.SS "template<class Domain > typedef \fBDomain\fP \fBrl::ValueIterationController\fP< \fBDomain\fP >::\fBDomainT\fP"

.SS "template<class Domain > typedef \fBLearningController\fP<\fBDomainT\fP,\fBUtilityDataTypeT\fP> \fBrl::ValueIterationController\fP< \fBDomain\fP >::\fBLearningControllerT\fP"

.SS "template<class Domain > typedef \fBMappedUtility\fP<\fBStateT\fP> \fBrl::ValueIterationController\fP< \fBDomain\fP >::\fBMappedUtilityT\fP\fC [protected]\fP"

.SS "template<class Domain > typedef \fBMaxUtilityActionAlgorithm\fP<\fBStateT\fP,\fBActionT\fP> \fBrl::ValueIterationController\fP< \fBDomain\fP >::\fBMaxUtilityActionAlgorithmT\fP"

.SS "template<class Domain > typedef \fBPolicyT::PolicyConstPtrT\fP \fBrl::ValueIterationController\fP< \fBDomain\fP >::\fBPolicyConstPtrT\fP"

.SS "template<class Domain > typedef PolicyGenerationAlgorithmT::PolicyGenerationAlgorithmPtrT \fBrl::ValueIterationController\fP< \fBDomain\fP >::\fBPolicyGenerationAlgorithmPtrT\fP"

.SS "template<class Domain > typedef class \fBPolicyGenerationAlgorithm\fP< \fBStateT\fP, \fBActionT\fP, \fBUtilityDataTypeT\fP > \fBrl::ValueIterationController\fP< \fBDomain\fP >::\fBPolicyGenerationAlgorithmT\fP"

.SS "template<class Domain > typedef \fBPolicyT::PolicyPtrT\fP \fBrl::ValueIterationController\fP< \fBDomain\fP >::\fBPolicyPtrT\fP"

.SS "template<class Domain > typedef \fBLearningControllerT::PolicyT\fP \fBrl::ValueIterationController\fP< \fBDomain\fP >::\fBPolicyT\fP"

.SS "template<class Domain > typedef \fBSelectedReward\fP<\fBStateT\fP> \fBrl::ValueIterationController\fP< \fBDomain\fP >::\fBSelectedRewardT\fP"

.SS "template<class Domain > typedef \fBStateGeneratorT::StateGeneratorConstPtrT\fP \fBrl::ValueIterationController\fP< \fBDomain\fP >::\fBStateGeneratorConstPtrT\fP"

.SS "template<class Domain > typedef \fBStateGenerator\fP<\fBStateT\fP> \fBrl::ValueIterationController\fP< \fBDomain\fP >::\fBStateGeneratorT\fP"

.SS "template<class Domain > typedef \fBDomainT::StateT\fP \fBrl::ValueIterationController\fP< \fBDomain\fP >::\fBStateT\fP"

.SS "template<class Domain > typedef \fBTransitionT::TransitionConstPtrT\fP \fBrl::ValueIterationController\fP< \fBDomain\fP >::\fBTransitionConstPtrT\fP"

.SS "template<class Domain > typedef \fBTransition\fP<\fBStateT\fP,\fBActionT\fP> \fBrl::ValueIterationController\fP< \fBDomain\fP >::\fBTransitionT\fP"

.SS "template<class Domain > typedef \fBUtilityT::UtilityConstPtrT\fP \fBrl::ValueIterationController\fP< \fBDomain\fP >::\fBUtilityConstPtrT\fP"

.SS "template<class Domain > typedef float \fBrl::ValueIterationController\fP< \fBDomain\fP >::\fBUtilityDataTypeT\fP"

.SS "template<class Domain > typedef \fBUtilityT::UtilityPtrT\fP \fBrl::ValueIterationController\fP< \fBDomain\fP >::\fBUtilityPtrT\fP"

.SS "template<class Domain > typedef \fBLearningControllerT::UtilityT\fP \fBrl::ValueIterationController\fP< \fBDomain\fP >::\fBUtilityT\fP"

.SH "Constructor & Destructor Documentation"
.PP 
.SS "template<class Domain > \fBrl::ValueIterationController\fP< \fBDomain\fP >::\fBValueIterationController\fP (\fBDomainConstPtrT\fP _domain, float _defaultUtility, float _discount, float _maxErr, bool _train = \fCtrue\fP)\fC [inline]\fP, \fC [explicit]\fP"

.SS "template<class Domain > virtual \fBrl::ValueIterationController\fP< \fBDomain\fP >::~\fBValueIterationController\fP ()\fC [inline]\fP, \fC [virtual]\fP"

.SS "template<class Domain > \fBrl::ValueIterationController\fP< \fBDomain\fP >::\fBValueIterationController\fP ()\fC [inline]\fP, \fC [protected]\fP"

.SH "Member Function Documentation"
.PP 
.SS "template<class Domain > virtual short \fBrl::ValueIterationController\fP< \fBDomain\fP >::finishedLearning () const\fC [inline]\fP, \fC [virtual]\fP"

.PP
\fBReturn values:\fP
.RS 4
\fI-2\fP the learning can never be finished because the system was not initialized 
.br
\fI-1\fP the learning process has not converged yet 
.br
\fI0\fP it is not known whether the learning has converged yet\&. This can only be determined by checking back with the domain and evaluating there whether the learning has finished (for example, in Q-Learning)\&. This function will therefore always return 0 for this Controller implementation\&. 
.br
\fI1\fP the learning has converged\&. 
.RE
.PP

.PP
Implements \fBrl::LearningController< Domain, float >\fP\&.
.SS "template<class Domain > virtual \fBActionT\fP \fBrl::ValueIterationController\fP< \fBDomain\fP >::getBestAction (const \fBStateT\fP & currentState) const\fC [inline]\fP, \fC [protected]\fP, \fC [virtual]\fP"
returns the best action to perform given the current state, at the current stage of the learning process\&. For online learners, this will be the action currently recommended\&. For offline learners, this will be the optimal action from the policy learned at initialisation (\fBlearnOffline()\fP)\&. 
.PP
Implements \fBrl::LearningController< Domain, float >\fP\&.
.SS "template<class Domain > virtual \fBPolicyConstPtrT\fP \fBrl::ValueIterationController\fP< \fBDomain\fP >::getPolicy () const\fC [inline]\fP, \fC [virtual]\fP"
Return the learned policy 
.PP
Implements \fBrl::LearningController< Domain, float >\fP\&.
.SS "template<class Domain > virtual \fBUtilityConstPtrT\fP \fBrl::ValueIterationController\fP< \fBDomain\fP >::getUtility () const\fC [inline]\fP, \fC [virtual]\fP"
Return the learned utility function 
.PP
Implements \fBrl::LearningController< Domain, float >\fP\&.
.SS "template<class Domain > virtual bool \fBrl::ValueIterationController\fP< \fBDomain\fP >::initializeImpl (const \fBStateT\fP & startState)\fC [inline]\fP, \fC [protected]\fP, \fC [virtual]\fP"

.PP
Implements \fBrl::LearningController< Domain, float >\fP\&.
.SS "template<class Domain > virtual bool \fBrl::ValueIterationController\fP< \fBDomain\fP >::isOnlineLearner ()\fC [inline]\fP, \fC [virtual]\fP"
If this method returns true, we will only have to call \fBinitialize()\fP in order to learn the utility function\&. After it has been initialised, the function transferState() can be used to transfer the state of the domain and thus use the learned policy to move around in the world\&. 
.PP
Implements \fBrl::LearningController< Domain, float >\fP\&.
.SS "template<class Domain > virtual bool \fBrl::ValueIterationController\fP< \fBDomain\fP >::learnOffline (const \fBStateT\fP & currState)\fC [inline]\fP, \fC [protected]\fP, \fC [virtual]\fP"
If the implementing algorithm is an online method, this function should return true and do nothing\&. 
.PP
Reimplemented from \fBrl::LearningController< Domain, float >\fP\&.
.SS "template<class Domain > virtual void \fBrl::ValueIterationController\fP< \fBDomain\fP >::printValues (std::ostream & o) const\fC [inline]\fP, \fC [virtual]\fP"
This will print the relevant values for the learning algorithm, e\&.g\&. the learned utility, policy, or q-table\&. This will vary between implementations\&. 
.PP
Implements \fBrl::LearningController< Domain, float >\fP\&.
.SS "template<class Domain > virtual void \fBrl::ValueIterationController\fP< \fBDomain\fP >::resetStartState (const \fBStateT\fP & startState)\fC [inline]\fP, \fC [virtual]\fP"
This method can be used to set the start state to follow with the optimal policy\&. This means that no connection with the previous state passed to updateAndGetBestAction() is assumed any more, which may be important for some online learning algorithms\&. (therefore, this method is pure virtual, to make sure subclasses consider the case that the state may be reset)\&. 
.PP
Implements \fBrl::LearningController< Domain, float >\fP\&.

.SH "Author"
.PP 
Generated automatically by Doxygen for LearningAlgorithms from the source code\&.
